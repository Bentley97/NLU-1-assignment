{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FirstAssignment.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO3dBuRzhB546Eb4+nQ5LQd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Bentley97/NLU_First_assignment/blob/main/FirstAssignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWWgrJ7wRxTu"
      },
      "source": [
        "# **FIRST ASSIGNMENT**\n",
        "\n",
        "Student:\n",
        "*   Name: Luca\n",
        "*   Surname: Bentivoglio\n",
        "*   Student number: 221246\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KPiVxYKzaE-C"
      },
      "source": [
        "Hoping it will be appriciated, I created a Colab notebook because I had some problems installing spaCy on my personal laptop.\n",
        "\n",
        "Anyway, I put code and report for each function all together in the notebook. If you want to run it, you should be able to open it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I50Ll8o8U8TE"
      },
      "source": [
        "## **REQUIREMENTS**\n",
        "\n",
        "If you open the notebook, is sufficient to run the first block of code to import spaCy, define a sentence for tests and load the dependency parsing model.\n",
        "Then you can run each function with its test calls."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JrieWP0k6tnq"
      },
      "source": [
        "import spacy\n",
        "\n",
        "test_sentence = \"I saw a man with a telescope.\"\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-PkeTbma8yZ"
      },
      "source": [
        "## **FUNCTION 1**  -  `extract_dependency_path(sentence)`\n",
        "### Extract a path of dependency relations from the ROOT to a token.\n",
        "\n",
        "This function parses the sentence given in input and extracts for each token the dependency path from the ROOT to the token itself.\n",
        "This is accomplished, starting from the token, exploring backwards arcs in the dependency graph/tree thanks to the attribute `head` that gives the parent of the token, until it reaches the ROOT, and the attribute `dep_` that gives the dependency relation between the token and its parent.\n",
        "\n",
        "\n",
        "Input:\n",
        "* sentence ==> a string\n",
        "\n",
        "Output:\n",
        "* the list of paths\n",
        "\n",
        "Each path is structured as follows:\n",
        "\n",
        "['token1', '--dependencyRelation1->', 'token2', '--dependencyRelation2->', 'token3']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kq4wKemN6MTT"
      },
      "source": [
        "def extract_dependency_path(sentence):\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  paths = []\n",
        "\n",
        "  for token in doc:\n",
        "    token_path = []\n",
        "\n",
        "    while token != token.head:  #we can use also (token != token.sent.root)\n",
        "      token_path.insert(0, token.text)\n",
        "      token_path.insert(0, \"--\"+token.dep_+\"->\")      #another format:  token_path.insert(0, \"goes with dep '\"+token.dep_+\"' in\")\n",
        "      token = token.head\n",
        "\n",
        "    token_path.insert(0, token.text)\n",
        "    token_path.insert(0, \"--\"+token.dep_+\"->\")        #another format:  token_path.insert(0, \"goes with dep '\"+token.dep_+\"' in\")\n",
        "    token_path.insert(0, \"ROOT\")\n",
        "    paths.append(token_path)\n",
        "  \n",
        "  return paths\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h98bo_XKqWyG",
        "outputId": "ce1a7286-4856-4722-f55e-6961fb417297"
      },
      "source": [
        "dep_paths = extract_dependency_path(test_sentence)\n",
        "\n",
        "for dep_path in dep_paths:\n",
        "  print(\"The dependency path for token '{}'\\tis:\\t{}\".format(dep_path[-1],dep_path).expandtabs(15))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dependency path for token 'I'            is:            ['ROOT', '--ROOT->', 'saw', '--nsubj->', 'I']\n",
            "The dependency path for token 'saw'          is:            ['ROOT', '--ROOT->', 'saw']\n",
            "The dependency path for token 'a'            is:            ['ROOT', '--ROOT->', 'saw', '--dobj->', 'man', '--det->', 'a']\n",
            "The dependency path for token 'man'          is:            ['ROOT', '--ROOT->', 'saw', '--dobj->', 'man']\n",
            "The dependency path for token 'with'         is:            ['ROOT', '--ROOT->', 'saw', '--dobj->', 'man', '--prep->', 'with']\n",
            "The dependency path for token 'a'            is:            ['ROOT', '--ROOT->', 'saw', '--dobj->', 'man', '--prep->', 'with', '--pobj->', 'telescope', '--det->', 'a']\n",
            "The dependency path for token 'telescope'    is:            ['ROOT', '--ROOT->', 'saw', '--dobj->', 'man', '--prep->', 'with', '--pobj->', 'telescope']\n",
            "The dependency path for token '.'            is:            ['ROOT', '--ROOT->', 'saw', '--punct->', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pY7NFqi-ayhp"
      },
      "source": [
        "## **FUNCTION 2**  -  `extract_subtrees_of_dependents(sentence)`\n",
        "### Extract subtree of a dependents given a token \n",
        "\n",
        "This function  parses the sentence in input and for each token extracts the subtree of its dependents ordered w.r.t. sentence order.\n",
        "\n",
        "Premising that I think a subtree is not a tree without its root, I did not understand clearly what is the request, since returning simply the subtree attribute seeemed to me too trivial.\n",
        "So I wrote two functions:\n",
        "1. `extract_subtrees_of_dependents(sentence)`: this one extracts for each token its subtree (ordered w.r.t sentence order) removing the token itself, since in the description of the requirement there is \"extract a subtree of its dependents\", so I extracted only the dependents of the token ordered w.r.t sentence order;\n",
        "1. `extract_subtrees(sentence)`: this one extracts the complete subtree of each token ordered w.r.t sentence order.\n",
        "\n",
        "Input:\n",
        "* sentence ==> a string\n",
        "\n",
        "Output:\n",
        "* a list of lists containing the token and the relative subtree  ([token, subtree])\n",
        "\n",
        "The output element is structured as follows:\n",
        "\n",
        "[\n",
        "  [token1, [tokenA, tokenB, tokenC]],\n",
        "  [token2, [tokenA, tokenB, tokenC]],\n",
        "  [token3, [tokenA, tokenB, tokenC]]\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ne2jh62axs8"
      },
      "source": [
        "def extract_subtrees_of_dependents(sentence):\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  result = []\n",
        "  \n",
        "  for token in doc:\n",
        "    subtree_list = []\n",
        "    for element in token.subtree:\n",
        "      if element != token:\n",
        "        subtree_list.append(element)\n",
        "    result.append([token, subtree_list])\n",
        "\n",
        "  return result  #returns a list of elements, each composed by the token and the list of its dependents in its subtree(without himself) ordered w.r.t. sentence order\n",
        "\n",
        "\n",
        "def extract_subtrees(sentence):\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  result = []\n",
        "\n",
        "  for token in doc:\n",
        "    result.append([token, list(token.subtree)])\n",
        "\n",
        "  return result\n"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IodsDpSjnpeN",
        "outputId": "e688fccf-c89d-40d0-abd5-7610fd87e70d"
      },
      "source": [
        "subtrees = extract_subtrees_of_dependents(test_sentence)\n",
        "print(\"Function: extract_subtrees_of_dependents\")\n",
        "for t,s in subtrees:\n",
        "  print(\"Token: {}\\t{}\".format(t,s).expandtabs(20))\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "\n",
        "subtrees = extract_subtrees(test_sentence)\n",
        "print(\"Function: extract_subtrees\")\n",
        "for t,s in subtrees:\n",
        "  print(\"Token: {}\\t{}\".format(t,s).expandtabs(20))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Function: extract_subtrees_of_dependents\n",
            "Token: I            []\n",
            "Token: saw          [I, a, man, with, a, telescope, .]\n",
            "Token: a            []\n",
            "Token: man          [a, with, a, telescope]\n",
            "Token: with         [a, telescope]\n",
            "Token: a            []\n",
            "Token: telescope    [a]\n",
            "Token: .            []\n",
            "\n",
            "\n",
            "\n",
            "Function: extract_subtrees\n",
            "Token: I            [I]\n",
            "Token: saw          [I, saw, a, man, with, a, telescope, .]\n",
            "Token: a            [a]\n",
            "Token: man          [a, man, with, a, telescope]\n",
            "Token: with         [with, a, telescope]\n",
            "Token: a            [a]\n",
            "Token: telescope    [a, telescope]\n",
            "Token: .            [.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mg7vogKaqBoL"
      },
      "source": [
        "## **FUNCTION 3**  -  `is_subtree(sentence, segment)`\n",
        "### Check if a given list of tokens (segment of a sentence) forms a subtree\n",
        "\n",
        "The function `is_subtree(sentence, segment)` checks if a segment(ordered list of words) in input forms a subtree of the sentence in input.\n",
        "It calls `extract_subtrees(sentence)` to extract subtree of each token of the sentence, cycle over all subtrees, search a corrispondence between the segment(list of words) and subtree(list of tokens) checking at every turn if they are equal.\n",
        "(Here I considered the concept of subtree as complete with the root)\n",
        "\n",
        "Input:\n",
        "* sentence ==> a string\n",
        "* segment ==> list of strings\n",
        "\n",
        "Output:\n",
        "* boolean: True if the segment forms a subtree of the sentence, False otherwise\n",
        "\n",
        "The equality between the segment(list of strings) and the subtree(list of Tokens) is given by the function `is_equal(tokenList, stringList)` that checks the length of the two and if each word of the segment is equal to each text of token, basically if the two lists have the same ordered words.\n",
        "\n",
        "Input:\n",
        "* tokenList ==> list of tokens\n",
        "* stringList ==> list of strings\n",
        "\n",
        "Output:\n",
        "* boolean: True if tokenList and stringList have the same ordered words, False otherwise"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzcZeoMrqE5P"
      },
      "source": [
        "def is_equal(tokenList, stringList):  #check equality between elements of a list of tokens and a list of strings\n",
        "  if len(tokenList) != len(stringList):\n",
        "    return False\n",
        "\n",
        "  for i in range(len(tokenList)):\n",
        "    if tokenList[i].text != stringList[i]:\n",
        "      return False\n",
        "  \n",
        "  return True\n",
        "\n",
        "\n",
        "def is_subtree(sentence, segment): #segment => ordered list of words from a sentence\n",
        "  subtrees = extract_subtrees(sentence)\n",
        "\n",
        "  for token, subtree in subtrees:\n",
        "    if(is_equal(subtree, segment)):\n",
        "        return True\n",
        "\n",
        "  return False"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EjSLuyZUvZl2",
        "outputId": "afcaf67e-1c32-44d9-c74b-cb99db7f3d75"
      },
      "source": [
        "input_segment1 = [\"a\",\"man\",\"with\",\"a\",\"telescope\"]\n",
        "input_segment2 = [\"a\",\"man\"]\n",
        "input_segment3 = [\"with\",\"a\",\"telescope\"]\n",
        "\n",
        "print(\"TEST 1\")\n",
        "print(\"Sentence:\\t{}\".format(test_sentence))\n",
        "print(\"Segment:\\t{}\".format(input_segment1))\n",
        "print(is_subtree(test_sentence, input_segment1))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"TEST 2\")\n",
        "print(\"Sentence:\\t{}\".format(test_sentence))\n",
        "print(\"Segment:\\t{}\".format(input_segment2))\n",
        "print(is_subtree(test_sentence, input_segment2))\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"TEST 3\")\n",
        "print(\"Sentence:\\t{}\".format(test_sentence))\n",
        "print(\"Segment:\\t{}\".format(input_segment3))\n",
        "print(is_subtree(test_sentence, input_segment3))\n",
        "print(\"\\n\")"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TEST 1\n",
            "Sentence:\tI saw a man with a telescope.\n",
            "Segment:\t['a', 'man', 'with', 'a', 'telescope']\n",
            "True\n",
            "\n",
            "\n",
            "TEST 2\n",
            "Sentence:\tI saw a man with a telescope.\n",
            "Segment:\t['a', 'man']\n",
            "False\n",
            "\n",
            "\n",
            "TEST 3\n",
            "Sentence:\tI saw a man with a telescope.\n",
            "Segment:\t['with', 'a', 'telescope']\n",
            "True\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Kgar-tpn3-F"
      },
      "source": [
        "## **FUNCTION 4**  -  `get_head(span)`\n",
        "### Identify head of a span, given its tokens\n",
        "\n",
        "This function given in input a string rappresenting a span(sequence of words),  returns the head of the span.\n",
        "It simply parses the span and extracts its root, cycling over all token and checking if its dependency relation points to a root(or its head is itself).\n",
        "In case the span is contains two sentences or maybe a sequence of words which the parser recognize as more sentences, the function return only the first root that it meets.\n",
        "\n",
        "Input:\n",
        "* span ==> string\n",
        "\n",
        "Output:\n",
        "* string: text of the root of the span\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMNwSYyFn3hk"
      },
      "source": [
        "def get_head(span):     #identify head of a span\n",
        "  doc = nlp(span)\n",
        "\n",
        "  return [token for token in doc if token.head == token][0].text   # the condition can be changed with (token.dep_ == 'ROOT')  or  we can use also Span object of spacy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UexIDRuGxH9j",
        "outputId": "31c78d90-408a-4382-ae8d-89e42b4e7089"
      },
      "source": [
        "print(\"Test 1:\")\n",
        "span1 = \"Give it to me! Right now.\"   #to test return single word \n",
        "print(\"Span:\",span1)\n",
        "print(\"Head:\",get_head(span1))\n",
        "\n",
        "print(\"\\n\")\n",
        "\n",
        "print(\"Test 2:\")\n",
        "span2 = \"boowling water bad under down mountain\"\n",
        "print(\"Span:\",span2)\n",
        "print(\"Head:\",get_head(span2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test 1:\n",
            "Span: Give it to me! Right now.\n",
            "Head: Give\n",
            "\n",
            "\n",
            "Test 2:\n",
            "Span: boowling water bad under down mountain\n",
            "Head: water\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BObFGeUyhBc"
      },
      "source": [
        "## **FUNCTION 5**  -  `extract_spans(sentence)`\n",
        "### Extract sentence subject, direct object and indirect object spans\n",
        "\n",
        "This function given in input a string rappresenting a sentence, returns the spans for subject (`nsubj`), direct object (`dobj`) and indirect object (`dative`) if they are present, otherwise nothing.\n",
        "\n",
        "I used the label `dative` instead of `iobj` because I have printed all the labels in the trained pipeline `en_core_web_sm` with the command `nlp.get_pipe(\"tagger\").labels` and I have noticed that `iobj` is not present.\n",
        "After some web research I found that that pipeline uses ClearNLP labels where `iobj` is deprecated and has been replaced by `dative`.\n",
        "\n",
        "Reference: [ClearNLP](https://github.com/clir/clearnlp-guidelines/blob/master/md/specifications/dependency_labels.md)\n",
        "\n",
        "\n",
        "The function firstly parses the sentence, cycles over all tokens, checks if the dependency of the token is one of the required dependencies(`nsubj`, `dobj`, `dative`), if yes it fills a dict with texts of the elements of the subtree of that token.\n",
        "\n",
        "Input:\n",
        "* sentence ==> string\n",
        "\n",
        "Output:\n",
        "* dict: dict with a list of strings connected to the respective dependency\n",
        "\n",
        "The output dict is structured as follows:\n",
        "{\n",
        "  'nsubj':['word1', 'word2', 'word3'],\n",
        "  'dobj':['word1', 'word2', 'word3'],\n",
        "  'dative':['word1', 'word2', 'word3']\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqSedXP7ygns"
      },
      "source": [
        "def extract_spans(sentence):\n",
        "  doc = nlp(sentence)\n",
        "\n",
        "  span_dict = {\n",
        "      'nsubj':[],\n",
        "      'dobj':[],\n",
        "      'dative':[]\n",
        "  }\n",
        "\n",
        "  for token in doc:\n",
        "    if token.dep_ == 'nsubj' or token.dep_ == 'dobj' or token.dep_ == 'dative':\n",
        "      for descendant in token.subtree:                    # instead of cycle on elements of the subtree we can use left_edge and right_edge attributes that gives directly first and last element of the token's subtree\n",
        "        span_dict[token.dep_].append(descendant.text)     #span_dict[token.dep_] = doc[token.left_edge.i : token.right_edge.i+1]\n",
        "  \n",
        "  return span_dict\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DrkkEY_ZASBM",
        "outputId": "c859cf5e-8429-4d84-9b3c-f6e7e0347fc9"
      },
      "source": [
        "sentence = \"I saw a man with a telescope.\"\n",
        "\n",
        "spans = extract_spans(sentence)\n",
        "\n",
        "print(\"Sentence:\", sentence)\n",
        "print(\"Spans:\")\n",
        "for item in spans.items():\n",
        "  print(item)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sentence: I saw a man with a telescope.\n",
            "Spans:\n",
            "('nsubj', ['I'])\n",
            "('dobj', ['a', 'man', 'with', 'a', 'telescope'])\n",
            "('dative', [])\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}